{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEuh8pR5sJpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d68f50a-7a86-4783-8898-9b7781bb260b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  image_name     xtl     ytl     xbr     ybr  small  medium  large    raw  \\\n",
            "0   i100.jpg   88.12   96.68  140.07  180.90  False    True  False  False   \n",
            "1   i100.jpg   89.77  136.31  149.25  225.00  False   False   True  False   \n",
            "2   i100.jpg  132.19   80.20  175.90  151.60  False    True  False  False   \n",
            "3   i100.jpg  140.07   94.53  200.63  208.83  False   False   True  False   \n",
            "4   i100.jpg  188.45   73.75  209.59  107.79   True   False  False   True   \n",
            "\n",
            "    ripe  \n",
            "0   True  \n",
            "1   True  \n",
            "2   True  \n",
            "3   True  \n",
            "4  False  \n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Parse the XML file\n",
        "def parse_annotations(xml_file):\n",
        "    # Parse the XML tree\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # List to store the extracted data\n",
        "    data = []\n",
        "\n",
        "    # Iterate through each image in the XML\n",
        "    for image in root.findall('image'):\n",
        "        image_name = image.get('name')  # Get the image name\n",
        "\n",
        "        # Iterate through each box in the image\n",
        "        for box in image.findall('box'):\n",
        "            xtl = float(box.get('xtl'))  # Top-left x-coordinate\n",
        "            ytl = float(box.get('ytl'))  # Top-left y-coordinate\n",
        "            xbr = float(box.get('xbr'))  # Bottom-right x-coordinate\n",
        "            ybr = float(box.get('ybr'))  # Bottom-right y-coordinate\n",
        "\n",
        "            # Get attributes (e.g., size and ripeness)\n",
        "            attributes = {}\n",
        "            for attribute in box.findall('attribute'):\n",
        "                attributes[attribute.get('name')] = attribute.text == 'true'\n",
        "\n",
        "            # Add a row of data to the list\n",
        "            data.append({\n",
        "                'image_name': image_name,\n",
        "                'xtl': xtl,\n",
        "                'ytl': ytl,\n",
        "                'xbr': xbr,\n",
        "                'ybr': ybr,\n",
        "                'small': attributes.get('Small', False),\n",
        "                'medium': attributes.get('Medium', False),\n",
        "                'large': attributes.get('Large', False),\n",
        "                'raw': attributes.get('Raw', False),\n",
        "                'ripe': attributes.get('Ripe', False),\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Example usage\n",
        "xml_file = \"annotations.xml\"  # Replace with the path to your annotation file\n",
        "annotations_df = parse_annotations(xml_file)\n",
        "\n",
        "# Display the parsed data\n",
        "print(annotations_df.head())\n",
        "\n",
        "# Save the data to a CSV file for future use\n",
        "annotations_df.to_csv(\"parsed_annotations.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kdMpNWBZ-fa",
        "outputId": "adbbdc60-7907-406e-aac6-48974ad3d52f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "# from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define the preprocessing transform for MiDaS\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),                # Convert the image to PIL format\n",
        "    transforms.Resize((384, 384)),          # Resize image to 384x384 (MiDaS input size)\n",
        "    transforms.ToTensor(),                  # Convert image to PyTorch tensor\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],        # Normalize using ImageNet means\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "\n",
        "\n",
        "# Load the MiDaS model\n",
        "def load_midas_model():\n",
        "    model_type = \"DPT_Large\"  # Choose a MiDaS model\n",
        "    midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
        "    midas.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    midas.eval()\n",
        "\n",
        "    # Define MiDaS transform\n",
        "    transform = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").dpt_transform\n",
        "    return midas, transform\n",
        "\n",
        "# Estimate depth for an image\n",
        "def estimate_depth(image_path, midas, transform):\n",
        "    # Check if the image file exists\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
        "\n",
        "    # Read the image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "\n",
        "    # Convert to RGB\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Transform for MiDaS model\n",
        "    input_batch = transform(img).unsqueeze(0).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    # print(f\"Image shape before transform: {img.shape}\")\n",
        "    # print(f\"Input batch shape: {input_batch.shape}\")\n",
        "\n",
        "\n",
        "    # Predict depth\n",
        "    with torch.no_grad():\n",
        "        depth = midas(input_batch)\n",
        "        depth = torch.nn.functional.interpolate(\n",
        "            depth.unsqueeze(1),\n",
        "            size=img.shape[:2],\n",
        "            mode=\"bicubic\",\n",
        "            align_corners=False,\n",
        "        ).squeeze().cpu().numpy()\n",
        "\n",
        "    return depth\n",
        "    # return 0\n",
        "\n",
        "def extract_depth_data(df, midas, transform):\n",
        "    depth_data = []\n",
        "\n",
        "    for image_name in df['image_name'].unique():\n",
        "        image_path = f\"{image_name}\"\n",
        "\n",
        "        print(f\"Processing {image_name}...\")\n",
        "        try:\n",
        "            # Generate depth map\n",
        "            depth_map = estimate_depth(image_path, midas, transform)\n",
        "            print(f\"Depth map shape for {image_name}: {depth_map.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating depth map for {image_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Filter rows for the current image\n",
        "        image_rows = df[df['image_name'] == image_name]\n",
        "        print(f\"Bounding boxes for {image_name}: {len(image_rows)}\")\n",
        "        if image_rows.empty:\n",
        "            print(f\"No valid bounding boxes found for {image_name}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        for _, row in image_rows.iterrows():\n",
        "            try:\n",
        "                # Extract and validate bounding box coordinates\n",
        "                xtl, ytl, xbr, ybr = map(int, [row['xtl'], row['ytl'], row['xbr'], row['ybr']])\n",
        "                xtl = max(0, xtl)\n",
        "                ytl = max(0, ytl)\n",
        "                xbr = min(depth_map.shape[1], xbr)\n",
        "                ybr = min(depth_map.shape[0], ybr)\n",
        "\n",
        "                # Extract depth for bounding box\n",
        "                bbox_depth = depth_map[ytl:ybr, xtl:xbr]\n",
        "                if bbox_depth.size == 0:  # Handle empty depth slices\n",
        "                    print(f\"Empty depth slice for bounding box in {image_name}. Skipping...\")\n",
        "                    depth_data.append(np.nan)\n",
        "                    continue\n",
        "\n",
        "                avg_depth = np.mean(bbox_depth)\n",
        "                depth_data.append(avg_depth)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing bounding box for {row['image_name']}: {e}\")\n",
        "                depth_data.append(np.nan)\n",
        "\n",
        "    # Ensure the length of depth_data matches the DataFrame\n",
        "    print(f\"Total depth values collected: {len(depth_data)}\")\n",
        "    if len(depth_data) != len(df):\n",
        "        raise ValueError(f\"Length of depth_data ({len(depth_data)}) does not match the number of rows in df ({len(df)}).\")\n",
        "\n",
        "    df['depth'] = depth_data\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "ZZtstHK2bN6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "midas, transform = load_midas_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXXT4nI9bcpv",
        "outputId": "e8d43207-2006-482f-cc63-79009789091f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image_dir = \"path_to_images\"  # Replace with the directory containing your images\n",
        "xml_file = \"annotations.xml\"\n",
        "annotations_df = parse_annotations(xml_file)\n",
        "annotations_df = extract_depth_data(annotations_df, midas, transform)\n",
        "\n",
        "# Save updated data with depth\n",
        "annotations_df.to_csv(\"annotations_with_depth.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO42TVdxdj7f",
        "outputId": "7d213c77-f3b9-4165-8972-f1ea12d6d5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing i100.jpg...\n",
            "Image shape before transform: (225, 225, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i100.jpg: (225, 225)\n",
            "Bounding boxes for i100.jpg: 6\n",
            "Processing i101.jpg...\n",
            "Image shape before transform: (175, 288, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i101.jpg: (175, 288)\n",
            "Bounding boxes for i101.jpg: 1\n",
            "Processing i102.jpg...\n",
            "Image shape before transform: (183, 275, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i102.jpg: (183, 275)\n",
            "Bounding boxes for i102.jpg: 4\n",
            "Processing i103.jpg...\n",
            "Image shape before transform: (163, 310, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i103.jpg: (163, 310)\n",
            "Bounding boxes for i103.jpg: 4\n",
            "Processing i104.jpg...\n",
            "Image shape before transform: (194, 260, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i104.jpg: (194, 260)\n",
            "Bounding boxes for i104.jpg: 7\n",
            "Processing i105.jpg...\n",
            "Image shape before transform: (260, 260, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i105.jpg: (260, 260)\n",
            "Bounding boxes for i105.jpg: 1\n",
            "Processing i106.jpg...\n",
            "Image shape before transform: (612, 490, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i106.jpg: (612, 490)\n",
            "Bounding boxes for i106.jpg: 1\n",
            "Processing i107.jpg...\n",
            "Image shape before transform: (1148, 1300, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i107.jpg: (1148, 1300)\n",
            "Bounding boxes for i107.jpg: 1\n",
            "Processing i108.jpg...\n",
            "Image shape before transform: (667, 1000, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i108.jpg: (667, 1000)\n",
            "Bounding boxes for i108.jpg: 1\n",
            "Processing i109.jpg...\n",
            "Image shape before transform: (489, 626, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i109.jpg: (489, 626)\n",
            "Bounding boxes for i109.jpg: 1\n",
            "Processing i110.jpg...\n",
            "Image shape before transform: (627, 600, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i110.jpg: (627, 600)\n",
            "Bounding boxes for i110.jpg: 1\n",
            "Processing i111.jpg...\n",
            "Image shape before transform: (894, 894, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i111.jpg: (894, 894)\n",
            "Bounding boxes for i111.jpg: 4\n",
            "Processing i112.jpg...\n",
            "Image shape before transform: (1280, 576, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i112.jpg: (1280, 576)\n",
            "Bounding boxes for i112.jpg: 16\n",
            "Processing i113.jpg...\n",
            "Image shape before transform: (576, 1280, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i113.jpg: (576, 1280)\n",
            "Bounding boxes for i113.jpg: 8\n",
            "Processing i114.jpg...\n",
            "Image shape before transform: (576, 1280, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i114.jpg: (576, 1280)\n",
            "Bounding boxes for i114.jpg: 7\n",
            "Processing i115.jpg...\n",
            "Image shape before transform: (576, 1280, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i115.jpg: (576, 1280)\n",
            "Bounding boxes for i115.jpg: 8\n",
            "Processing i116.jpg...\n",
            "Image shape before transform: (576, 1280, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i116.jpg: (576, 1280)\n",
            "Bounding boxes for i116.jpg: 10\n",
            "Processing i117.jpg...\n",
            "Image shape before transform: (275, 183, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i117.jpg: (275, 183)\n",
            "Bounding boxes for i117.jpg: 3\n",
            "Processing i118.jpg...\n",
            "Image shape before transform: (260, 260, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i118.jpg: (260, 260)\n",
            "Bounding boxes for i118.jpg: 1\n",
            "Processing i119.jpg...\n",
            "Image shape before transform: (894, 894, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i119.jpg: (894, 894)\n",
            "Bounding boxes for i119.jpg: 4\n",
            "Processing i120.jpg...\n",
            "Image shape before transform: (667, 1000, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i120.jpg: (667, 1000)\n",
            "Bounding boxes for i120.jpg: 1\n",
            "Processing i121.jpg...\n",
            "Image shape before transform: (612, 490, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i121.jpg: (612, 490)\n",
            "Bounding boxes for i121.jpg: 1\n",
            "Processing i122.jpg...\n",
            "Image shape before transform: (894, 894, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i122.jpg: (894, 894)\n",
            "Bounding boxes for i122.jpg: 4\n",
            "Processing i123.jpg...\n",
            "Image shape before transform: (489, 626, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i123.jpg: (489, 626)\n",
            "Bounding boxes for i123.jpg: 1\n",
            "Processing i124.jpg...\n",
            "Image shape before transform: (275, 183, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i124.jpg: (275, 183)\n",
            "Bounding boxes for i124.jpg: 3\n",
            "Processing i125.jpg...\n",
            "Image shape before transform: (612, 490, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i125.jpg: (612, 490)\n",
            "Bounding boxes for i125.jpg: 1\n",
            "Processing i126.jpg...\n",
            "Image shape before transform: (225, 225, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i126.jpg: (225, 225)\n",
            "Bounding boxes for i126.jpg: 22\n",
            "Processing i127.jpg...\n",
            "Image shape before transform: (260, 260, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i127.jpg: (260, 260)\n",
            "Bounding boxes for i127.jpg: 1\n",
            "Processing i128.jpg...\n",
            "Image shape before transform: (489, 626, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i128.jpg: (489, 626)\n",
            "Bounding boxes for i128.jpg: 1\n",
            "Processing i129.jpg...\n",
            "Image shape before transform: (894, 894, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i129.jpg: (894, 894)\n",
            "Bounding boxes for i129.jpg: 4\n",
            "Processing i130.jpg...\n",
            "Image shape before transform: (667, 1000, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i130.jpg: (667, 1000)\n",
            "Bounding boxes for i130.jpg: 1\n",
            "Processing i131.jpg...\n",
            "Image shape before transform: (627, 600, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i131.jpg: (627, 600)\n",
            "Bounding boxes for i131.jpg: 1\n",
            "Processing i86.jpg...\n",
            "Image shape before transform: (183, 275, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i86.jpg: (183, 275)\n",
            "Bounding boxes for i86.jpg: 20\n",
            "Processing i87.jpg...\n",
            "Image shape before transform: (148, 341, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i87.jpg: (148, 341)\n",
            "Bounding boxes for i87.jpg: 2\n",
            "Processing i88.jpg...\n",
            "Image shape before transform: (259, 194, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i88.jpg: (259, 194)\n",
            "Bounding boxes for i88.jpg: 4\n",
            "Processing i89.jpg...\n",
            "Image shape before transform: (225, 225, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i89.jpg: (225, 225)\n",
            "Bounding boxes for i89.jpg: 3\n",
            "Processing i90.jpg...\n",
            "Image shape before transform: (205, 246, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i90.jpg: (205, 246)\n",
            "Bounding boxes for i90.jpg: 26\n",
            "Processing i91.jpg...\n",
            "Image shape before transform: (195, 258, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i91.jpg: (195, 258)\n",
            "Bounding boxes for i91.jpg: 23\n",
            "Processing i92.jpg...\n",
            "Image shape before transform: (183, 275, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i92.jpg: (183, 275)\n",
            "Bounding boxes for i92.jpg: 3\n",
            "Processing i93.jpg...\n",
            "Image shape before transform: (183, 275, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i93.jpg: (183, 275)\n",
            "Bounding boxes for i93.jpg: 5\n",
            "Processing i94.jpg...\n",
            "Image shape before transform: (275, 183, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i94.jpg: (275, 183)\n",
            "Bounding boxes for i94.jpg: 12\n",
            "Processing i95.jpg...\n",
            "Image shape before transform: (225, 225, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i95.jpg: (225, 225)\n",
            "Bounding boxes for i95.jpg: 3\n",
            "Processing i96.jpg...\n",
            "Image shape before transform: (194, 259, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i96.jpg: (194, 259)\n",
            "Bounding boxes for i96.jpg: 7\n",
            "Processing i97.jpg...\n",
            "Image shape before transform: (153, 330, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i97.jpg: (153, 330)\n",
            "Bounding boxes for i97.jpg: 1\n",
            "Processing i98.jpg...\n",
            "Image shape before transform: (194, 259, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i98.jpg: (194, 259)\n",
            "Bounding boxes for i98.jpg: 14\n",
            "Processing i99.jpg...\n",
            "Image shape before transform: (168, 300, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i99.jpg: (168, 300)\n",
            "Bounding boxes for i99.jpg: 3\n",
            "Processing image (1).jpg...\n",
            "Image shape before transform: (1066, 1300, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for image (1).jpg: (1066, 1300)\n",
            "Bounding boxes for image (1).jpg: 17\n",
            "Processing image (1)_.jpg...\n",
            "Image shape before transform: (225, 225, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for image (1)_.jpg: (225, 225)\n",
            "Bounding boxes for image (1)_.jpg: 5\n",
            "Processing image (2).jpg...\n",
            "Image shape before transform: (190, 265, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for image (2).jpg: (190, 265)\n",
            "Bounding boxes for image (2).jpg: 2\n",
            "Processing image (2)_.jpg...\n",
            "Image shape before transform: (217, 232, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for image (2)_.jpg: (217, 232)\n",
            "Bounding boxes for image (2)_.jpg: 4\n",
            "Processing image (3).jpg...\n",
            "Image shape before transform: (232, 217, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for image (3).jpg: (232, 217)\n",
            "Bounding boxes for image (3).jpg: 1\n",
            "Processing image.jpg...\n",
            "Image shape before transform: (183, 275, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for image.jpg: (183, 275)\n",
            "Bounding boxes for image.jpg: 7\n",
            "Total depth values collected: 292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# depth_map = estimate_depth(\"i100.jpg\", midas, transform)\n",
        "# print(f\"Depth map shape for i100.jpg:\", depth_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMm_7HAhgj1F",
        "outputId": "8aafa5dc-d51e-4403-d06f-bf4a21982801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape before transform: (225, 225, 3)\n",
            "Input batch shape: torch.Size([1, 3, 384, 384])\n",
            "Depth map shape for i100.jpg: [[ 2.8744266  2.9286826  2.9766912 ... 19.163998  18.961834  19.074747 ]\n",
            " [ 2.8878372  2.9264984  2.9923728 ... 19.141748  19.08182   18.957941 ]\n",
            " [ 2.8944275  2.9323015  2.9825191 ... 19.013908  18.978682  18.924757 ]\n",
            " ...\n",
            " [ 8.989193   9.018615   9.039868  ... 11.592698  11.58648   11.596172 ]\n",
            " [ 8.980381   9.045133   9.090462  ... 11.661094  11.604401  11.5635805]\n",
            " [ 8.993718   9.041263   9.053713  ... 11.695225  11.575205  11.518712 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Area and depth proportionality"
      ],
      "metadata": {
        "id": "RJIjgOwhoUAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"annotations_with_depth.csv\")\n",
        "\n",
        "# Calculate bounding box area\n",
        "df[\"area\"] = (df[\"xbr\"] - df[\"xtl\"]) * (df[\"ybr\"] - df[\"ytl\"])\n",
        "\n",
        "# Calculate proportionality metric\n",
        "df[\"metric\"] = df[\"area\"] / df[\"depth\"]\n",
        "\n",
        "# Save the updated dataframe for inspection\n",
        "df.to_csv(\"annotations_with_area_and_metric.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "G8HVdsO3huvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimating break points"
      ],
      "metadata": {
        "id": "SVlSmE_0oqV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Filter rows for each size class\n",
        "small_metrics = df[df[\"small\"] == True][\"metric\"]\n",
        "medium_metrics = df[df[\"medium\"] == True][\"metric\"]\n",
        "large_metrics = df[df[\"large\"] == True][\"metric\"]\n",
        "\n",
        "# # Calculate breakpoints\n",
        "# break_point1 = np.max(small_metrics)  # Highest value for small\n",
        "# break_point2 = np.min(large_metrics)  # Lowest value for large\n",
        "\n",
        "# # Print the results\n",
        "# print(f\"Break Point 1 (Small-Medium): {break_point1}\")\n",
        "# print(f\"Break Point 2 (Medium-Large): {break_point2}\")\n"
      ],
      "metadata": {
        "id": "2sVnI66Noeae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Calculate breakpoints based on the mean metric of small and large\n",
        "# break_point1 = np.mean(small_metrics)  # Mean of small category\n",
        "# break_point2 = np.mean(large_metrics)  # Mean of large category\n",
        "\n",
        "# print(f\"Break Point 1 (Small-Medium): {break_point1}\")\n",
        "# print(f\"Break Point 2 (Medium-Large): {break_point2}\")\n",
        "\n",
        "\n",
        "\n",
        "# Calculate medians instead of using min/max\n",
        "break_point1 = small_metrics.median()  # Median for small category\n",
        "break_point2 = large_metrics.median()  # Median for large category\n",
        "\n",
        "print(f\"Break Point 1 (Small-Medium): {break_point1}\")\n",
        "print(f\"Break Point 2 (Medium-Large): {break_point2}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz8_-uGHpBtQ",
        "outputId": "2dfd3760-5d03-46f3-a5df-7c20f61db810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Break Point 1 (Small-Medium): 32.32600853009189\n",
            "Break Point 2 (Medium-Large): 59.09826083014485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify mangoes based on breakpoints\n",
        "def classify_size(metric, break_point1, break_point2):\n",
        "    if metric < break_point1:\n",
        "        return \"Small\"\n",
        "    elif break_point1 <= metric < break_point2:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Large\"\n",
        "\n",
        "# Apply the classification to the DataFrame\n",
        "df[\"predicted_size\"] = df[\"metric\"].apply(lambda x: classify_size(x, break_point1, break_point2))\n",
        "df_filtered = df[[\"image_name\", \"small\", \"medium\", \"large\", \"predicted_size\"]]\n",
        "# Save the results to a new CSV file\n",
        "df.to_csv(\"annotations_with_predicted_size.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "xVKxyvajpTD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the actual size label from the annotations\n",
        "def get_actual_size(row):\n",
        "    if row['small']:\n",
        "        return 'Small'\n",
        "    elif row['medium']:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "# Apply to the DataFrame to get the actual size\n",
        "df_filtered[\"actual_size\"] = df_filtered.apply(get_actual_size, axis=1)\n",
        "\n",
        "# Calculate the accuracy\n",
        "correct_predictions = (df_filtered[\"predicted_size\"] == df_filtered[\"actual_size\"]).sum()\n",
        "total_predictions = len(df_filtered)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYzr6oYApdYP",
        "outputId": "11f3e9b0-91d8-42d8-dffe-1bad962b18fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 40.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-e1ffef3b66d1>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered[\"actual_size\"] = df_filtered.apply(get_actual_size, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ZXgvGZOq4tx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}